# 3.2 上下文管理 (Context Management)

**目标:** 学习如何管理对话的上下文，计算 Token 消耗，并实现简单的历史记录修剪策略，防止 Token 溢出。

## 1. 原理讲解

Gemini (以及所有 LLM) 都是**无状态**的。它们记不住你之前说过的话。为了让模型有“记忆”，我们需要在每次请求时，将之前的对话历史 (History) 一并发送给模型。

### 1.1 `Content` 对象
在 Google Gemini API 中，每条消息都是一个 `Content` 对象：
```typescript
interface Content {
  role: 'user' | 'model';
  parts: Part[];
}

interface Part {
  text?: string;
  // ... 其他类型 (图片、函数调用等)
}
```

### 1.2 Token 限制
每个模型都有上下文窗口限制 (Context Window)。
- `gemini-1.5-pro`: 2,000,000 Tokens (巨大)
- `gemini-2.0-flash`: 1,000,000 Tokens (依然很大，但不是无限)

即便现在的模型窗口很大，我们依然需要管理 Token：
1.  **成本**: 更多的 Token = 更高的费用 (虽然 Flash 目前免费额度很高)。
2.  **延迟**: 输入越多，处理越慢。
3.  **如果超出限制**: 请求会失败。

---

## 2. 实战教程

### 教程 1: 手动管理历史记录 (Manual History)

虽然 `startChat` 很好用，但了解底层原理很重要。我们将演示如何自己维护一个数组。

**创建文件:** `packages/core/src/examples/context-manual.ts`

```typescript
import { GoogleGenAI, Content } from '@google/genai';
import * as dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';

const __dirname = path.dirname(fileURLToPath(import.meta.url));
dotenv.config({ path: path.resolve(__dirname, '../../.env') });

const apiKey = process.env.GEMINI_API_KEY!;
const genAI = new GoogleGenAI(apiKey);
const model = genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });

// 手动维护历史记录
const history: Content[] = [];

async function chat(message: string) {
  // 1. 构造用户消息并添加
  const userContent: Content = { role: 'user', parts: [{ text: message }] };
  history.push(userContent);

  console.log(`User: ${message}`);

  // 2. 发送请求 (带上所有历史)
  // 注意：generateContentStream 的 contents 参数接收整个数组
  const result = await model.generateContentStream({
    contents: history,
  });

  process.stdout.write('Gemini: ');
  let responseText = '';
  
  for await (const chunk of result.stream) {
    const text = chunk.text();
    process.stdout.write(text);
    responseText += text;
  }
  console.log('\n');
  
  // 3. 将模型的回复也加入历史
  // 注意：必须手动添加，否则下一次请求就没有这条回复了
  history.push({ role: 'model', parts: [{ text: responseText }] });
}

async function main() {
  await chat("你好，我有一个苹果。");
  await chat("我又买了一个香蕉。");
  await chat("我现在有什么水果？"); 
}

main().catch(console.error);
```

### 教程 2: 计算 Token 数量 (Token Counting)

SDK 提供了 `countTokens` 方法来预估 Token 数量。

**创建文件:** `packages/core/src/examples/token-count.ts`

```typescript
import { GoogleGenAI } from '@google/genai';
import * as dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';

const __dirname = path.dirname(fileURLToPath(import.meta.url));
dotenv.config({ path: path.resolve(__dirname, '../../.env') });

async function main() {
  const genAI = new GoogleGenAI(process.env.GEMINI_API_KEY!);
  const model = genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });

  const text = "你好，Gemini。这是一个测试 Token 数量的句子。";
  
  // 1. 计算文本的 Token
  const countResult = await model.countTokens(text);
  console.log(`文本: "${text}"`);
  console.log(`Token 数量: ${countResult.totalTokens}`); // 对于中文，通常 1个汉字 > 1 Token

  // 2. 模拟一个较长的历史记录
  const longHistory = [
    { role: 'user', parts: [{ text: '我的代码为什么报错了？' }] },
    { role: 'model', parts: [{ text: '请提供错误日志。' }] },
    { role: 'user', parts: [{ text: 'Error: NullPointerException at ... (此处省略1000字)' }] }
  ];

  // 计算历史记录的总 Token
  const historyCount = await model.countTokens({ contents: longHistory });
  console.log(`\n历史记录总 Token: ${historyCount.totalTokens}`);
}

main().catch(console.error);
```

### 教程 3: 实现简单的上下文裁剪 (Context Pruning)

当 Token 接近限制时，我们需要丢弃最早的对话。这是一个简单的先进先出 (FIFO) 策略。
**注意**: 永远不要删除 `System Instruction` (如果它是作为第一条消息传递的话)。但在 `gemini-api` 中，`systemInstruction` 是单独字段，所以只需管理 `history` 数组即可。

**创建文件:** `packages/core/src/examples/context-pruning.ts`

```typescript
import { GoogleGenAI, Content } from '@google/genai';
import * as dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';

const __dirname = path.dirname(fileURLToPath(import.meta.url));
dotenv.config({ path: path.resolve(__dirname, '../../.env') });

const genAI = new GoogleGenAI(process.env.GEMINI_API_KEY!);
const model = genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });

// 模拟一个最大 Token 限制 (实际模型要大得多，这里为了演示设小一点)
const MAX_TOKENS = 60; 

async function pruneHistory(history: Content[]): Promise<Content[]> {
  let currentTokens = (await model.countTokens({ contents: history })).totalTokens;

  if (currentTokens <= MAX_TOKENS) {
    return history;
  }

  console.log(`⚠️ 当前 Token (${currentTokens}) 超过限制 (${MAX_TOKENS})，开始裁剪...`);
  
  // 复制一份，避免修改原数组
  const newHistory = [...history];

  // 循环删除最早的一轮对话 (User + Model)，直到满足要求
  // 注意：我们通常希望保持第一轮对话（如果是设定的背景），但在简单FIFO中直接删最旧的
  while (currentTokens > MAX_TOKENS && newHistory.length > 1) {
    // 删除第一条 (可能是 User)
    const removed1 = newHistory.shift();
    // 如果下一条是 Model，通常也要一起删，保持对话成对 (User -> Model)
    // 但在 Gemini API 中，不成对也可以，不过逻辑上最好成对删除
    if (newHistory.length > 0 && newHistory[0].role === 'model') {
       newHistory.shift();
    }

    // 重新计算 (可以在本地估算以减少 API 调用，但用 API 最准)
    currentTokens = (await model.countTokens({ contents: newHistory })).totalTokens;
    console.log(`  - 裁剪后剩余 Token: ${currentTokens}`);
  }

  return newHistory;
}

async function main() {
  let history: Content[] = [
    { role: 'user', parts: [{ text: '第一句话。' }] }, // 假设 5 tokens
    { role: 'model', parts: [{ text: '这是第一句的回答。' }] }, // 假设 10 tokens
    { role: 'user', parts: [{ text: '第二句话，稍微长一点。' }] }, // 假设 10 tokens
    { role: 'model', parts: [{ text: '这是第二句的回答。' }] }, // 假设 10 tokens
    { role: 'user', parts: [{ text: '第三句话，非常非常长，用来触发裁剪机制。1234567890' }] }, // 假设 20 tokens
  ];

  console.log('--- 裁剪前 ---');
  console.log(`消息条数: ${history.length}`);
  
  history = await pruneHistory(history);

  console.log('\n--- 裁剪后 ---');
  console.log(`消息条数: ${history.length}`);
  console.log('剩余内容:', JSON.stringify(history, null, 2));
}

main().catch(console.error);
```

## 3. 作业

1.  **运行代码**: 依次运行三个示例 (`context-manual.ts`, `token-count.ts`, `context-pruning.ts`)。
2.  **阅读源码**: (可选) 查看 `gemini-cli` 中的 `ChatCompressionService` (`packages/core/src/services/chatCompressionService.ts`)。看看真实的生产级项目是如何做"压缩"的。它不仅仅是删除旧消息，还会让模型把旧的对话总结成一段摘要！

通过这一节的学习，你已经掌握了如何让 AI 助手拥有“长期记忆”并控制“脑容量”。下一节我们将进入最激动人心的部分：**Tool Calling (工具调用)**。
